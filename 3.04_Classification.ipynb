{"cells":[{"cell_type":"markdown","metadata":{"id":"Jq9GPMWyPlu3"},"source":["# **Classification**"]},{"cell_type":"markdown","metadata":{"id":"ld2Xt3tXPq1W"},"source":["## **Agenda**"]},{"cell_type":"markdown","metadata":{"id":"xAmn9_6uPswa"},"source":["In this lesson, we will cover the following concepts of classification with the help of a business use case:\n","* Linear vs. nonlinear classifiers\n","* Naive Baye's theorem\n","* Support vector machines\n","* Evaluating the model using accuracy and confusion matrix"]},{"cell_type":"markdown","metadata":{"id":"WyCHg0Y2L0nu"},"source":["## **Linear vs. Nonlinear Classifiers**\n","\n","### **Linear Classifiers**\n","1. Linear classifiers use a linear combination of input features to categorize data into labels.\n","2. They use the data to establish a linear boundary.\n","3. They use a line, plane, or hyperplane (a plane that exists in more than two dimensions) to distinguish data.\n","4. They can be used to classify data that is linearly separable. \n","5. They can be modified to classify data that is not linearly separable.\n","6. Some examples of linear classifier are: \n","  * Perceptron\n","  * Linear regression\n","  * Multiple linear regression\n","  * Logistic regression\n","  * Support vector machine (SVM) (with linear kernel)\n","  * Linear discriminant classifier (LDC)\n","  * Naive Baye's Theorem\n","  * Gradient descent\n","\n","<br/>\n","\n","### **Nonlinear Classifiers**\n","1. Nonlinear classifiers use a nonlinear combination of input features to categorize data into labels.\n","2. They use the data to establish a nonlinear boundary.\n","3. They classify data by mapping it into a high-dimensional space.\n","4. They are used to classify complex data which cannot be distinguished by a simple threshold or linear boundary.\n","5. They cannot be modified to classify data that is linearly separable.\n","6. Some examples of nonlinear classifiers are:\n","  * Multilayer perceptron\n","  * K-nearest neighbors (KNN)\n","  * Decision trees\n","  * Random forest\n","  * Discriminant classifier\n","  * Quadratic discriminant classifier (QDC)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1L4FyNaGKSCV"},"source":["##### **Naive Baye's Theorem**"]},{"cell_type":"markdown","metadata":{"id":"DtQIVIIYKYjn"},"source":["Naive Baye's theorem is used in classifications that assume that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G0rXf01ULPlK"},"source":["###### **Naive Baye's Theorem**"]},{"cell_type":"markdown","metadata":{"id":"cunLjmaLLT-x"},"source":["![NB1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB1.JPG)"]},{"cell_type":"markdown","metadata":{"id":"9T-FPFIiLscQ"},"source":["###### **Example**"]},{"cell_type":"markdown","metadata":{"id":"2UONqRvqLfAB"},"source":["* As a first step toward prediction using Naive Baye's theorem, you will have to estimate the frequency of each and every attribute.\n","\n","  ![NB2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB2.JPG)"]},{"cell_type":"markdown","metadata":{"id":"J92M15zpMB8W"},"source":["* Calculating the likelihood of each attribute: \n","\n","  ![NB3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB3.JPG)\n","  <br><br><br>\n","\n","  ![NB4](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB4.JPG)"]},{"cell_type":"markdown","metadata":{"id":"4OudA31wHmhz"},"source":["* Let us find the probability of playing golf under the following conditions:\n","\t* Outlook \t\t=\tRain \n","\t* Humidity \t\t=\tHigh\n","\t* Wind\t\t\t=\tWeak\n","\t* Play\t\t\t=\t?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fMLf1jAyerEN"},"source":["* Solution:\n","\n","  * Calculation:\n","  \n","  ![NB5](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB5.JPG)\n","\n","  * Prediction:\n","  \n","  ![NB6](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/NB6.JPG)"]},{"cell_type":"markdown","metadata":{"id":"i8zguNNkjdw2"},"source":["##### **Support Vector Machine**"]},{"cell_type":"markdown","metadata":{"id":"kyzSieayjfsT"},"source":["Let us understand the following basics before moving on to the mathematics behind SVM:\n","* Linear Separators\n","* Optimal Separation\n","* Classification Margin"]},{"cell_type":"markdown","metadata":{"id":"aWriMf77kCiS"},"source":["**1. Linear Separators:**\n","\n","Consider a binary separation which can be viewed as the task of separating classes in the feature space.\n","\n","![SVM1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM1.JPG)"]},{"cell_type":"markdown","metadata":{"id":"OjfeiIcbnWjH"},"source":["**2. Optimal Separation:**\n","\n","Classification becomes difficult in the presence of multiple separators. Therefore, it is important to have an optimal separator.\n","\n","\n","![SVM2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM2.JPG)"]},{"cell_type":"markdown","metadata":{"id":"-X0Te0GtqrA8"},"source":["**3. Classification margin:**\n","\n","* Concept of classification margin:\n","Let's use the diagram below, to understand a separator and classification margin in a better manner.\n","\n","![SVM3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM3.JPG)\n","\n","* Need for maximizing the classification margin:\n","  * It generalizes the predictions and performs better on the test data by not overfitting the model to the training data.\n","  * It takes care of the support vectors, ignoring other training examples.\n","\n","![SVM4](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM4.JPG)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xDHZ9o_psJLF"},"source":["###### **Linear SVM**"]},{"cell_type":"markdown","metadata":{"id":"Om9MyZRAsjK1"},"source":["![SVM5](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM5.JPG)"]},{"cell_type":"markdown","metadata":{"id":"49nQh2hAst4h"},"source":["* Formulate the quadratic optimization problem: \n","\n","  ![SVM6](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM6.JPG)\n","\n","\n","* Reformulate the problem as:\n","\n","  ![SVM7](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM7.JPG)"]},{"cell_type":"markdown","metadata":{"id":"T3nfw_l5vBgp"},"source":["###### **Nonlinear SVM**"]},{"cell_type":"markdown","metadata":{"id":"-Y8uiyucvEkr"},"source":["  ![SVM8](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM8.JPG)"]},{"cell_type":"markdown","metadata":{"id":"GIlGfFBBvwbb"},"source":["**Feature Spaces:**\n","\n","  ![SVM9](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM9.JPG)\n","\n","\n","The original feature space can always be mapped to some higher-dimensional feature space where the training set is separable.\n"]},{"cell_type":"markdown","metadata":{"id":"WT5FDv5aGOex"},"source":["###### **Kernel Trick**"]},{"cell_type":"markdown","metadata":{"id":"u5XWaqFCGRoi"},"source":["* The linear classifier relies on inner product between vectors K(xi,xj)=xiTxj.\n","\n","* If every datapoint is mapped into high-dimensional space via some transformation **Φ:  x → φ(x)**, the inner product becomes<br>\n","\n","    **K(xi,xj)= φ(xi) Tφ(xj)**\n","    <br>\n","\n","* A **kernel function** is a function that is equivalent to an inner product in a feature space.\n","<br>\n","\n","* **Example:** \n","\t\n","  ![SVM10](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.7_Supervised_Learning_-_Regression_and_Classification/Trainer_PPT_and_IPYNB/0.4_Classification/SVM10.JPG)\n","\n","* Thus, a kernel function implicitly maps data to a high-dimensional space (without the need to compute each φ(x) explicitly)."]},{"cell_type":"markdown","metadata":{"id":"J0VUxTtt350L"},"source":["## **Use Case: Classification**\n","\n","Note: With the help of a use case, we will perform all the basic steps to reach the model training and prediction part.\n","  "]},{"cell_type":"markdown","metadata":{"id":"0hUF8ZkRZ2uR"},"source":["### **Problem Statement**"]},{"cell_type":"markdown","metadata":{"id":"QvVPZv40aBEe"},"source":["\n","Our aim in this project is to predict if a person would buy an iPhone with respect to their gender, age, and income. We will also compare different classification algorithms..\n"]},{"cell_type":"markdown","metadata":{"id":"Sps4Q1eQaBMM"},"source":["### **Dataset**"]},{"cell_type":"markdown","metadata":{"id":"fh4uXSMPaCH7"},"source":["Before reading the data, you need to download \"iphone_purchase_records.csv\" dataset from the link given below and upload it to the Lab. We will use Up arrow icon which is shown in the left side under View icon. Click on the Up arrow icon and upload the file wherever it is downloaded in your system.\n","\n","After this, you will see the downloaded file on the left side of your lab with all the .pynb files.\n","\n","Link: https://www.dropbox.com/sh/br7x7c7gjaghibp/AAAYI4aI1u2oG4fqiVKmn-9Ua?dl=0\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"e424reU_aG5X"},"source":["### **Solution**\n","\n","Note: In logistic regression, we have used PCA for feature selection, but let us take another route this time."]},{"cell_type":"markdown","metadata":{"id":"mUFBvcXVbuwq"},"source":["#### **Import Libraries**\n","\n","In python, Pandas is used for data manipulation and analysis. Numpy is a package which includes a multidimensional array object as well as a number of derived objects. Matplotlib is an amazing visualization library in Python for 2D plots of arrays. Seaborn is an open-source Python library built on top of matplotlib.\n","\n","These libraries are written with the import keyword.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTDf-xoobpKZ"},"outputs":[],"source":["#import required libraries\n","import pandas as pd\n","from pandas import Series, DataFrame\n","\n","#import required libraries for visualization\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"z0MmdZaPbkXI"},"source":["#### **Data Acquisition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EchC-WqObq22"},"outputs":[],"source":["# Step 1 - Load Data\n","import pandas as pd\n","data_set = pd.read_csv(\"./iphone_purchase_records.csv\")\n","X = data_set.iloc[:,:-1].values\n","y = data_set.iloc[:, 3].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8wfqaL4btQ9"},"outputs":[],"source":["#Preview the train data\n","data_set.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18L8BwE7ncgu"},"outputs":[],"source":["#Check the data type\n","data_set.dtypes"]},{"cell_type":"markdown","metadata":{"id":"GNDiv7Zfq1s9"},"source":["#### **Feature Extraction**\n","\n","In the below code, you are using the sklearn library, which contains a lot of tools for machine learning and statistical modeling, including classification, regression, clustering, and dimensionality reduction. "]},{"cell_type":"markdown","metadata":{"id":"6nY7hw-JKhqd"},"source":["**1. Use LabelEncoder to convert gender to number**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWLxCtnq150y"},"outputs":[],"source":["from sklearn import preprocessing\n","# Step 2 - Convert Gender to number\n","from sklearn.preprocessing import LabelEncoder\n","labelEncoder_gender =  LabelEncoder()\n","X[:,0] = labelEncoder_gender.fit_transform(X[:,0])\n","\n","# Optional - if you want to convert X to float data type\n","import numpy as np\n","X = np.vstack(X[:, :]).astype(np.float)"]},{"cell_type":"markdown","metadata":{"id":"TEPtmr17uGnA"},"source":["#### **Splitting Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWgoE00_YPHv"},"outputs":[],"source":["# Step 3 - Split data into training and testing\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"IVX2enqc9rX8"},"source":["#### **Feature Scaling**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrhKqAeVmrT2"},"outputs":[],"source":["# Step 4 - Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wp8LMljVoKCW"},"outputs":[],"source":["# Step 5 - Logistic regression classifier\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state=0, solver=\"liblinear\")\n","classifier.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nRB0wKCoOS9"},"outputs":[],"source":["# Step 6 - Predicting logistic regression model on x_test\n","y_pred = classifier.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"fCoh62I8vE_v"},"source":["##### **Confusion Matrix**\n","\n","It can be used to find the number of correct and incorrect entries.\n","\n","*   If an individual has not purchased an iPhone and the expected value states that they have not purchased, it is a true negative (TN), i.e., the actual value is 0 and the predicted value is also 0.\n","\n","*   If an individual has not purchased an iPhone but the expected value states that they have, it is a false positive (FP), i.e., the actual value is 0 and the value expected is 1.\n","\n","*   If an individual has purchased an iPhone but the expected value states that they have not, it is a false negative (FN), i.e., the real value is 1 and the value expected is 0.\n","\n","*   If an individual has purchased an iPhone and the expected value also says that they have purchased it is True Positive (TP), i.e., the actual value is 1 and the predicted value is also 1.\n","\n","**Accuracy score:** This is the most common metric that is used to verify model accuracy. In other words, it is the proportion of the overall number of accurate predictions to the total number of predictions.\n","\n","**Accuracy score = (TP+TN)/(TP+TN+FP+FN)**\n","\n","**Recall score:** It is the proportion of positive incidents that we correctly expected. \n","\n","**Recall score = TP/(TP+FN)**\n","\n","**Precision score:** This is the proportion of positive outcomes expected that are currently positive. \n","\n","**Precision score = TP/(TP+FP)**\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wK8Tz3koTyA"},"outputs":[],"source":["# Step 7 - Confusion matrix\n","from sklearn import metrics\n","cm = metrics.confusion_matrix(y_test, y_pred) \n","print(cm)\n","accuracy = metrics.accuracy_score(y_test, y_pred) \n","print(\"Accuracy score:\",accuracy)\n","precision = metrics.precision_score(y_test, y_pred) \n","print(\"Precision score:\",precision)\n","recall = metrics.recall_score(y_test, y_pred) \n","print(\"Recall score:\",recall)\n"]},{"cell_type":"markdown","metadata":{"id":"lOCFg8Cfv-ka"},"source":["#### The model has a 91 % accuracy score, an 89 % precision score, and an 81 % recall score, indicating that it works effectively. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNhqwS6-puFc"},"outputs":[],"source":["# Step 8 - Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X = sc.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"SmN0HbK0u70c"},"source":["### **Model Comparison**\n","\n","Note: Comparing the accuracy score of all the classifier models that we used above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olHCYAF4p5Oz"},"outputs":[],"source":["# Step 9 - Compare classification algorithms\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LeIg9oMqC-N"},"outputs":[],"source":["classification_models = []\n","classification_models.append(('Logistic Regression', LogisticRegression(solver=\"liblinear\")))\n","classification_models.append(('K Nearest Neighbor', KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\",p=2)))\n","classification_models.append(('Kernel SVM', SVC(kernel = 'rbf',gamma='scale')))\n","classification_models.append(('Naive Bayes', GaussianNB()))\n","classification_models.append(('Decision Tree', DecisionTreeClassifier(criterion = \"entropy\")))\n","classification_models.append(('Random Forest', RandomForestClassifier(n_estimators=100, criterion=\"entropy\")))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMCO1i8DqJBC"},"outputs":[],"source":["for name, model in classification_models:\n","  kfold = KFold(n_splits=10, random_state=(7), shuffle=(True))\n","  result = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n","  print(\"%s: Mean Accuracy = %.2f%% - SD Accuracy = %.2f%%\" % (name, result.mean()*100, result.std()*100))"]},{"cell_type":"markdown","metadata":{"id":"djHxWXPLEzVs"},"source":["### **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"lCZVbbnDEsCT"},"source":["From the results, we can see that KNN and Kernel SVM have done better than the others for this particular dataset. So, we will shortlist these two for this project. This is precisely the same result that we arrived at by independently applying each of those algorithms."]},{"cell_type":"markdown","metadata":{"id":"jjzXJkJ0YmsT"},"source":["**Note: In this topic, we saw the use of the classification methods and in this lesson we have covered the concepts of supervised learning based on regression and classification, but in the next lesson we will be working on \"Decision Trees and Random Forest\".**"]},{"cell_type":"markdown","metadata":{"id":"ZlmPd32prmX-"},"source":["![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3.04_Classification.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}